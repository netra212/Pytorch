# -*- coding: utf-8 -*-
"""001_PyTorch_WorkFlow_ZTM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ll5PlWLKCHBmMA689mpYGZ6EwUpxFGmu

### **PyTorch Workflow**
"""

what_were_covering = {
    1: "data (prepare and local)",
    2: "build model",
    3: "fitting the model to data(training)",
    4: "making predictions and evaluting a model(inference)",
    6: "putting it all together"
}
print(what_were_covering)

import torch
from torch import nn # nn contains all of PyTorch's building blocks for neural networks
import matplotlib.pyplot as plt

# Check PyTorch Version
torch.__version__

"""### **Creating a Simple Dataset Using the Linear Regression Formula**

Data can be almost anything.

* Excel spreadsheet.
* Images of any kind.
* Videos (YouTube has lots of data...)
* Audio like songs or podcasts.
* DNA.
* Text.

Machine Learning is a game of two parts:
1. Get data into a numerical representations.
2. Building model to learn patterns in that numerical representations.
"""

# Linear Regression.
# Create *known* parameters.
import torch

weight = 0.7
bias = 0.3

# Create
start = 0
end = 1
step = 0.02
X = torch.arange(start, end, step).unsqueeze(dim=1)
y = weight * X + bias

X[:10], y[:10]

len(X), len(y)

# Generalization: The ability for a machine learning model to perform well on data it has't seen before.

# Splitting the data into the training, validation and test datasets.
# Now, Let's create a training and test set with our data.

# Create a train/test split.
train_split = int(0.8 * len(X))
X_train, y_train = X[:train_split],y[:train_split]
X_test, y_test = X[train_split:], y[train_split:]
len(X_train), len(X_test), len(y_train), len(y_test)

"""### **Visualize Data**"""

import matplotlib.pyplot as plt

def plot_predictions(train_data=X_train,
                     train_labels=y_train,
                     test_data=X_test,
                     test_labels=y_test,
                     predictions=None):
    """
    Plot training data, test data and compares predictions.
    """
    plt.figure(figsize=(10,7))

    # Plot training data in blue.
    plt.scatter(train_data, train_labels, c='b', s=4, label='Training data')

    # Plot testing data in green
    plt.scatter(test_data, test_labels, c='g', s=4, label='Testing data')

    # Are there predictions ?
    if predictions is not None:
        # Plot the predictions if they exist.
        plt.scatter(test_data, predictions, c='r', s=4, label='Predictions')

    # show the legend.
    plt.legend(prop={"size":14})

plot_predictions(X_train,y_train,X_test, y_test)

"""### **Creating our First PyTorch Model for Linear Regression**

What our model does:

* Start with random values (weights and bias)
* Look at training data and ajust the randonm values to better represent( or get closer to) the ideal values (the weight &bias values we used to create the data)

How does it do so ?

Through two main algorithms:

1. Gradient Descent.
2. Backpropagation.
"""

from torch import nn

# Create Linear Regression model class.
class LinearRegressionModel(nn.Module):# <- almost everything in PyTorch inherit from nn.Module
    def __init__(self):
        super().__init__()
        self.weights = nn.Parameter(torch.randn(1, # <- start with a random weights and try to adjust it to the ideal weights.
                                                requires_grad=True, # <- can this parameter be updated via graident descent ?
                                                dtype=torch.float)) # <- PyTorch loves the datatype torch.float32

        self.bias = nn.Parameter(torch.randn(1,  # <- start with a random weights and try to adjust it to the ideal weights.
                                             requires_grad=True, # <- can this parameter be updated via graident descent ?
                                             dtype=torch.float)) # <- PyTorch loves the datatype torch.float32

    # Forward method to define computation in the Model.
    # <- "x" is the inpout data
    def forward(self, x:torch.Tensor) -> torch.Tensor:
        return self.weights * x + self.bias

"""### **PyTorch Model Building essentials**

* `torch.nn` - contains all the building for computational graphs (a neural network can be considered as a computation graphs).
* `torch.nn`.Parameter - what parameters should our model try and learn, often a PyTorch layer from torch.nn will se thse for us.
* `torch.nn.Module` - The base class for all neural networks moduls, if you subclass it, you should overwrite forward()
* `torch.optim` - this is where the optimizers in PyTorch live, whey will help with gradient descent.
* `def foward()` - All nn.Module subclasses require you to overwrite forward(), this methods defines what happens in forward computation.

### **Checking the contents of our PyTorch Model**

So, we can check out model parameters or what's inside our model using `.parameter()`.
"""

# Create a random seed
torch.manual_seed(42)

# Create an instance of the model (this is a subclass of nn.Module)
model_0 = LinearRegressionModel()

# Check out the parameters.
list(model_0.parameters())

# List named parameters.
model_0.state_dict()

weight, bias

"""### **Making Predictions with Our Random Model Using Inference Mode**

`torch.inference_model()`

When we pass data through our model, it's going to run it through `forward()` method.
"""

X_test, y_test

y_preds = model_0(X_test)
y_preds

# Make predictions with model
# At the time of the prediction, we always implements the context manager.
with torch.inference_mode(): # inference_mode() Turn off the gradient tracking on training time or removes all of the gradients at the time of training.
# Benefits: PyTorch keep track the less data. so that prediction will be much more faster.
    y_preds = model_0(X_test)
y_preds

# We can also do something similar with torch.no_grad(), however, inference_model() is preferred.
with torch.no_grad():
    y_preds = model_0(X_test)
y_preds

"""`inference_mode()` :  Turn off the gradient tracking on training time or removes all of the gradients at the time of training.

Benefits: PyTorch keep track the less data. so that prediction will be much more faster.
"""

y_test

plot_predictions(predictions=y_preds)

"""### **Training the Model**

The whole idea of training for a model is to move from somne `*unkown*` parameters (these may be random) to some `*known*` parameters.

Or from poor representation of data to a better representation of the data.

One way to measure how poor or how wrong your models predictions are is to use a loss function.


* Note: Loss function may also be called cost function or criterion in different areas. For our case, we are going to refer to it as a loss function.

Things we need to train:

* **Loss Functions**: A function to measure how wrong your model's predictions to the ideal output.

* **Optimizer**: Takes into account the loss of a model and adjust the model's parameters (e.g. weights & bias) in our case, to improve the loss function.


And specifically for PyTorch, we need:
 * A training loop.
 * A testing loop.
"""

model_0.parameters()

list(model_0.parameters())

# Check out the model's parameters (a parameter is value that the model sets itself)
model_0.state_dict()

"""### **Implemeting the Loss functions**

-> Inside the Optimizer you'll often have to set two parameters:

 * `params` - the model parameters you'd like to optimize, for example `params=model_0.parameters()`
 * `lr` (learning rate) - the learning rate is a hyperparameter that defines how big/small the optimizer changes the parameters with each step (a small `lr` results in small changes, a large `lr` results in large changes).

"""

# Setup a loss function
loss_fn = nn.L1Loss()

# Setup an Optimizer. (Stochastic Gradient Descent)
optimizer = torch.optim.SGD(params=model_0.parameters(),
                            lr=0.01) # lr = learning rate
loss_fn, optimizer

# Optimizer adjust the model parameters.

"""**Q**: **Which loss function and optimizer should i use ?**

**A**: This will be problem specific. But with experience, you'll get an idea of what works and what does not with your particular problem set.

For example, for a regression problem (like ours), a loss function of `nn.LLoss()` and an optimizer like `torch.optim.SGD()` will suffice.

But for classification problem like classifying whether a photo is of a dog or a cat, you'll likely want to use a loss function of `nn.BCELoss()` (binary cross entropy loss).

### **Building a Training Loop (and a testing loop) in PyTorch**

Steps in a training loop:

* 0. Loop through the data.

* 1.  Forward Pass (this involves data moving through our model's `forward()` to make prediction on data - also called forward propagation.

* 2. Calculate the loss (compare forward pass predictions to ground truth labels)

* 3. Optimizer zero grad.

* 4. Loss backward - move backwards through the network to calculate the gradients of each of the parameters of our model with respect to the loss. (**backpropagation**)

* 5. Optimizer step - use the optimizer to adjust our model
's parameters to try and improve the loss. (**gradient descent**)
"""

model_0.state_dict()

list(model_0.parameters())

list(model_0.eval().parameters())

with torch.inference_mode():
    list(model_0.parameters())

with torch.no_grad():
    list(model_0.parameters())

"""Gradient : Gradient is a derivative of a function that has more than one input variables."""

torch.manual_seed(42)
# An epoch is one loop through the data....(this is a hyperparameter)
epochs = 500

# Track different values.
epoch_count = []
loss_values = []
test_loss_values = []

#### TRAINING.
# 0. Loop through the data.
for epoch in range(epochs):

    # Set the model to training data.
    model_0.train() # train mode in PyTorch sets all parameters that requires gradient to requires gradients.

    # 1. Forward Pass.
    y_pred = model_0(X_train)

    # 2. Calculate the Loss.
    loss = loss_fn(y_pred, y_train)
    # if epoch % 10 == 0:
    #     print(f'Training Loss: {loss}')

    # Optimizer zero grad.
    optimizer.zero_grad()

    # 4. Perform backpropagation on the loss with respect to the parameters of the model.
    loss.backward()

    # 5. Step the optimizer (Perform gradient descent)
    optimizer.step() # by default how the optimizer changes will accumulate through the loop. so ... we have to zero them above in step 3.

    ####. Testing
    model_0.eval()  # turn off the different settings in the model not needed for evaluation/testing purposes such as dropout/batch_normalization_layers.
    with torch.inference_mode(): # turn off the gradients tracking.
        # 1. Do the Forward Pass.
        test_pred = model_0(X_test)

        # 2. Calculate the loss.
        test_loss = loss_fn(test_pred, y_test)

    if epoch % 10 == 0:
        epoch_count.append(epoch)
        loss_values.append(loss)
        test_loss_values.append(test_loss)

        print(f"Epoch : {epoch} | Training Loss : {loss} | Test Loss: {test_loss}")

    # with torch.no_grad: # you may also see torch.no_grad() in older PyTorch Version.

    # print(model_0.state_dict())

print(epoch_count)
print(loss_values)
print(test_loss_values)

weight, bias

with torch.inference_mode():
    y_preds_new = model_0(X_test)

model_0.state_dict()

plot_predictions(predictions = y_preds)

plot_predictions(predictions = y_preds_new)

epoch_count, loss_values, test_loss_values

import numpy as np

np.array(torch.tensor(loss_values).cpu().numpy()), test_loss_values

# Plot the loss curves.
plt.plot(epoch_count, np.array(torch.tensor(loss_values).numpy()), label="Train Loss")
plt.plot(epoch_count, test_loss_values, label="Test Loss")
plt.title("Training and Test Loss Curves.")
plt.ylabel("Loss")
plt.xlabel("Epochs")
plt.legend()

"""### **Saving a Model in PyTorch**

There are three main methods we should know about for saving and loading models in PyTorch:

1. `torch.save()` - allows us to save a PyTorch Object in Python's Pickle format.

2. `torch.load()` - allows us to load a PyTorch Object.

3. `torch.nn.Module.load_state_dict()` - this allows us to load a model's saved state dictionary.
"""

model_0.state_dict()

# Saving our PyTorch Model.
from pathlib import Path

# 1. First create the model directory.
MODEL_PATH = Path("models")
MODEL_PATH.mkdir(parents=True, exist_ok=True)

# 2. Create Model save path.
MODEL_NAME = "01_pytorch_workflow_model_0.pt"
MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME

# 3. Save the Model state dict.
print(f'Saving Model to : {MODEL_SAVE_PATH}')
torch.save(obj=model_0.state_dict(),
           f=MODEL_SAVE_PATH)

!ls -l models

"""### **Loading PyTorch Models**

Since we have saved our model's `state_dict()` architecture rather the entire model, we will create the new instance of the model class and load the save `state_dict()` into that.
"""

model_0.state_dict()

# To load in a save state_dict(), we have instantiate the new instance of the model class.
loaded_model_0 = LinearRegressionModel()

# Load the save state_dict() of model_0 (this will update the new instance with updated parameters.)

loaded_model_0.state_dict()

model_0.state_dict()

loaded_model_0.load_state_dict(torch.load(f=MODEL_SAVE_PATH))

loaded_model_0.state_dict()

# Making some predictions with our loaded models.
model_0.eval() # evaluation mode.
with torch.inference_mode():
    loaded_models_preds = loaded_model_0(X_test)

loaded_models_preds

model_0.eval()
with torch.inference_mode():
    y_preds = model_0(X_test)
y_preds

# Compare loaded model preds with Original model preds.
y_preds == loaded_models_preds

import torch
x = torch.rand(5, 3)
print(x)

type(x)

"""## **Putting it all Together**"""

# Import PyTorch and Matplotlib
try:
    import torch
    from torch import nn
    import matplotlib.pyplot as plt

    print(f"Torch Version: {torch.__version__}")
except ModuleNotFoundError:
    print('Module not found Error.')

"""Create device-agnostic code.

This means if we've got access to a GPU, our code will use it (for potentially faster computing).

If no GPU is available, the code will default to using CPU.
"""

# Setup device agnostic code.
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

!nvidia-smi

"""#### **6.1 Data**"""

# Create some data using the linear regression formula of y = weight * X + bias.
weight = 0.7
baias = 0.3

# Create range values.
start = 0
end = 1
step = 0.02

# Create X and y (feature and labels)
X = torch.arange(start, end, step).unsqueeze(dim=1)# without unsqueeze, errors will popup.
y = weight * X + bias
X[:10], y[:10]

# Split data.
train_split = int(0.8 * len(X))
X_train, y_train = X[:train_split], y[:train_split]
X_test, y_test = X[train_split:], y[train_split:]

len(X_train), len(y_train), len(X_test), len(y_test)

import matplotlib.pyplot as plt

def plot_predictions(train_data=X_train,
                     train_labels=y_train,
                     test_data=X_test,
                     test_labels=y_test,
                     predictions=None):
    """
    Plot training data, test data and compares predictions.
    """
    plt.figure(figsize=(10,7))

    # Plot training data in blue.
    plt.scatter(train_data, train_labels, c='b', s=4, label='Training data')

    # Plot testing data in green
    plt.scatter(test_data, test_labels, c='g', s=4, label='Testing data')

    # Are there predictions ?
    if predictions is not None:
        # Plot the predictions if they exist.
        plt.scatter(test_data, predictions, c='r', s=4, label='Predictions')

    # show the legend.
    plt.legend(prop={"size":14})

plot_predictions(X_train,y_train,X_test, y_test)

"""#### **6.2 Building a PyTorch Linear Model**"""

# Create a Linear Model by subclassing nn.Module
class LinearRegressionModelV2(nn.Module):
    def __init__(self):
        super().__init__()

        # Use nn.Linear() for creating the model parameters / also called: linear transforms, probing layer, fully connected layer, dense layer in tensorflow. But all of them are implementing the linear layers.
        self.linear_layer = nn.Linear(in_features=1,
                                      out_features=1)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.linear_layer(x)

# Set the manual seed.
torch.manual_seed(42)
model_1 = LinearRegressionModelV2()
model_1, model_1.state_dict()

X_train[:2], y_train[:2]

"""#### **6.3 Training a Model**"""

# Check the model current device
next(model_1.parameters()).device

# Set the Model to use the target device.
model_1.to(device)

next(model_1.parameters()).device

"""For Training we need:

* Loss function.
* Optimizer.
* Training Loop.
* Testing Loop.
"""

# setup loss function.
loss_fn = nn.L1Loss() # same as MAE

# setup our optimizer.
optimizer = torch.optim.SGD(params=model_1.parameters(),
                            lr = 0.01)

# Let's write a training Loop.
torch.manual_seed(42)


# Put data on the target device (device agnostic code for the data).
X_train = X_train.to(device)
y_train = y_train.to(device)
X_test = X_test.to(device)
y_test = y_test.to(device)

epoch = 200

for epoch in range(epochs):
    model_1.train()

    # 1. Forward pass.
    y_pred = model_1(X_train)

    # 2. Calculate the loss.
    loss = loss_fn(y_pred, y_train)

    # 3. Optimizer zero grad.
    optimizer.zero_grad()

    # 4. Perform backpropagation.
    loss.backward()

    # 5. Optimizer step.
    optimizer.step()

    #### TESTING.
    # Now, the model is in the evaluation mode.
    # So, which turned off the batch normalization, dropout and so on.
    # Always call the eval() methods which is a good practice.
    model_1.eval()
    with torch.inference_mode(): # we don't need to track the gradients so we used the inference mode as context manager.
        test_pred = model_1(X_test)
        test_loss = loss_fn(test_pred, y_test)

    # Print out what's happening.

    if epoch % 10 == 0:
        print(f"Epoch: {epoch} | Loss: {loss} | Test Loss: {test_loss}")

model_1.state_dict()

weight, bias

"""#### **6.4 Making and Evaluating Predictions**"""

# Turn model into evaluation mode.
model_1.eval()

# Make predictions on the test data.
with torch.inference_mode():
    y_preds = model_1(X_test)
y_preds

# Check out our model predictions visually.
plot_predictions(predictions=y_preds.cpu())

"""#### **6.5 Saving & Loading a trained a Model**"""

from pathlib import Path

# 1. Create a models directroy
MODEL_PATH = Path("models")
MODEL_PATH.mkdir(parents=True, exist_ok=True)

# 2. Create model save path.
MODEL_NAME = "01_pytorch_workflow_model_1.pth"
MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME

# MODEL_SAVE_PATH

# 3. Save the Model state dict.
print(f"Saving model to : {MODEL_SAVE_PATH}")
torch.save(obj=model_1.state_dict(),
           f=MODEL_SAVE_PATH)

model_1.state_dict()

"""**Loading a PyTorch**"""

# Load a PyTorch model.

# Create a new instance of linear regression model V2.
loaded_model_1 = LinearRegressionModelV2()

# Load the saved model_1 state_dict
loaded_model_1.load_state_dict(torch.load(MODEL_SAVE_PATH))

# Put the loaded model to device.
loaded_model_1.to(device)

# Check in which device the parameters of the LinearRegressionModelV2() lies.
next(loaded_model_1.parameters()).device

# Evaluate loaded model.
loaded_model_1.eval()
with torch.inference_mode():
    loaded_model_1_preds = loaded_model_1(X_test)
y_preds == loaded_model_1_preds

